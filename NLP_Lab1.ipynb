{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaWbI9PqhmS8frAwB8+7cJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adriansuder/NLP_WSEI/blob/main/NLP_Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 1 BagOfWords"
      ],
      "metadata": {
        "id": "LiefWHBJ0zlF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GuXaIko0u_l",
        "outputId": "298eeb2d-476a-4761-f593-3bfe41ebe962"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pobrano 2368 dokumentów.\n",
            "\n",
            "Macierz Bag-of-Words:\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]]\n",
            "15 Najczęściej występujących słów:\n",
            "space: 1060\n",
            "edu: 835\n",
            "like: 743\n",
            "don: 722\n",
            "just: 642\n",
            "time: 623\n",
            "use: 606\n",
            "know: 594\n",
            "think: 560\n",
            "good: 548\n",
            "year: 546\n",
            "image: 530\n",
            "new: 488\n",
            "data: 487\n",
            "people: 476\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Download the stopwords for the 'polish' language\n",
        "\n",
        "# Wybieramy kilka kategorii do analizy\n",
        "categories = ['sci.med', 'sci.space', 'rec.sport.baseball', 'comp.graphics']\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "documents = newsgroups.data\n",
        "\n",
        "print(f\"Pobrano {len(documents)} dokumentów.\")\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "bow_matrix = vectorizer.fit_transform(documents)\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "\n",
        "X = bow_matrix.toarray()\n",
        "\n",
        "# Wyświetlenie macierzy Bag-of-Words\n",
        "print(\"\\nMacierz Bag-of-Words:\")\n",
        "print(X)\n",
        "\n",
        "# Sumowanie wystąpień słów w kolumnach\n",
        "word_counts = np.sum(X, axis=0)\n",
        "\n",
        "# Tworzenie listy par (słowo, liczebność)\n",
        "word_count_pairs = list(zip(vocabulary, word_counts))\n",
        "\n",
        "# Sortowanie listy według liczebności (malejąco)\n",
        "word_count_pairs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"15 Najczęściej występujących słów:\")\n",
        "for word, count in word_count_pairs[:15]:\n",
        "    print(f\"{word}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 2 tf-idf"
      ],
      "metadata": {
        "id": "LT862TYj06fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Wybieramy kilka kategorii do analizy\n",
        "categories = ['sci.med', 'sci.space', 'rec.sport.baseball', 'comp.graphics']\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "documents = newsgroups.data\n",
        "\n",
        "print(f\"Pobrano {len(documents)} dokumentów.\")\n",
        "\n",
        "# Inicjalizacja TfidfVectorizer z polskimi stop-słowami\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Pobranie słownika\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Wyświetlenie macierzy TF-IDF dla wszystkich dokumentów\n",
        "print(\"\\nMacierz TF-IDF:\")\n",
        "print(tfidf_matrix.toarray())\n",
        "\n",
        "#index dla którego wyświetlimy tf-idf\n",
        "doc_index = 2\n",
        "\n",
        "# Pobranie wartości TF-IDF dla wybranego dokumentu\n",
        "tfidf_values = tfidf_matrix[doc_index].toarray()[0]\n",
        "\n",
        "# Tworzenie DataFrame z słowami i ich wartościami TF-IDF\n",
        "tfidf_df = pd.DataFrame({'word': vocabulary, 'tfidf': tfidf_values})\n",
        "\n",
        "# Sortowanie DataFrame według wartości TF-IDF (malejąco)\n",
        "tfidf_df_sorted = tfidf_df.sort_values(by='tfidf', ascending=False)\n",
        "\n",
        "print(\"Wybrany dokument dla którego wyświetli się tf-idf\")\n",
        "print(documents[doc_index])\n",
        "\n",
        "# Wyświetlanie 10 słów o najwyższych wartościach TF-IDF\n",
        "print(f\"\\nSłowa o najwyższych wartościach TF-IDF dla dokumentu {doc_index}:\")\n",
        "print(tfidf_df_sorted.head(10))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWs3GS0U076d",
        "outputId": "d81b4063-ee95-4679-cf4f-7dc2724a344f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pobrano 2368 dokumentów.\n",
            "\n",
            "Macierz TF-IDF:\n",
            "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.13300203 0.         0.         ... 0.         0.         0.        ]]\n",
            "Wybrany dokument dla którego wyświetli się tf-idf\n",
            "\n",
            "\n",
            "Sjogren's syndrome has been known to induce dryness in vaginal tissue as well\n",
            "as induce primary biliary cirrhosis. Otherwise the abdominal swelling could be\n",
            "due to a complication of Sjogren's known as pseudolymphoma which *can* produce\n",
            "a splenomegaly (enlarged spleen). She should definitely see a rheumatologist.\n",
            "\n",
            "Since you don't mention skin disorder, anemia, or joint pain you'd probably\n",
            "rule out erythema nodosum or scleroderma.\n",
            "\n",
            "Josh\n",
            "backon@VMS.HUJI.AC.IL\n",
            "\n",
            "\n",
            "\n",
            "Słowa o najwyższych wartościach TF-IDF dla dokumentu 2:\n",
            "                 word     tfidf\n",
            "24003         sjogren  0.352447\n",
            "14074          induce  0.312464\n",
            "15390           known  0.187821\n",
            "18446         nodosum  0.176223\n",
            "5092          biliary  0.176223\n",
            "10459        erythema  0.176223\n",
            "24557    splenomegaly  0.176223\n",
            "9618          dryness  0.176223\n",
            "22495  rheumatologist  0.176223\n",
            "23271     scleroderma  0.176223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 3 word embdeddings"
      ],
      "metadata": {
        "id": "XsUpt5B_1AoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J687ddee1CSk",
        "outputId": "6964527a-15fc-4029-882d-7e7d5e0a0273"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, gensim\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from gensim.models import Word2Vec\n",
        "import string\n",
        "\n",
        "\n",
        "# Wybieramy kilka kategorii do analizy\n",
        "categories = ['sci.med', 'sci.space', 'rec.sport.baseball', 'comp.graphics']\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "documents = newsgroups.data\n",
        "\n",
        "print(f\"Pobrano {len(documents)} dokumentów.\")\n",
        "\n",
        "# Tokenizacja, usuwanie stop-słów i normalizacja\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
        "    return tokens\n",
        "\n",
        "processed_documents = [preprocess(doc) for doc in documents]\n",
        "\n",
        "# Trenowanie modelu Word2Vec\n",
        "model = Word2Vec(sentences=processed_documents, vector_size=100, window=5, min_count=5, workers=4)\n",
        "\n",
        "# Testowanie modelu, do word podajemy słowo dla którego znajdziemy podobne słowa\n",
        "word = \"space\"\n",
        "similar_words = model.wv.most_similar(word, topn=5)\n",
        "\n",
        "print(f\"Najbliższe słowa do '{word}':\")\n",
        "for similar_word, similarity in similar_words:\n",
        "    print(f\"{similar_word}: {similarity:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H11GPABo1EEY",
        "outputId": "7c7d07e1-adbe-4e59-f83e-ea1cf8deb388"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pobrano 2368 dokumentów.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Najbliższe słowa do 'space':\n",
            "center: 0.9885\n",
            "nasa: 0.9842\n",
            "national: 0.9819\n",
            "research: 0.9781\n",
            "data: 0.9760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 4 bigramy"
      ],
      "metadata": {
        "id": "tgpuMvaS1GIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "# Wybieramy kilka kategorii do analizy\n",
        "categories = ['sci.med', 'sci.space', 'rec.sport.baseball', 'comp.graphics']\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "documents = newsgroups.data\n",
        "\n",
        "print(f\"Pobrano {len(documents)} dokumentów.\")\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
        "    return tokens\n",
        "\n",
        "processed_documents = [preprocess(doc) for doc in documents]\n",
        "\n",
        "# Generowanie bigramów\n",
        "bigramy = []\n",
        "for dokument in processed_documents:\n",
        "  for i in range(len(dokument) - 1):\n",
        "    bigramy.append((dokument[i], dokument[i + 1]))\n",
        "\n",
        "licznik_bigramow = Counter(bigramy)\n",
        "\n",
        "# Wyświetlanie n najczęściej występujących bigramów\n",
        "print(f\"Najczęściej występujące bigramy (top 25):\")\n",
        "for bigram, liczba_wystapien in licznik_bigramow.most_common(25):\n",
        "    print(f\"{bigram}: {liczba_wystapien}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Smf9pAfb1Jvf",
        "outputId": "a991dd66-2458-495e-fd3a-b077e5972efb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pobrano 2368 dokumentów.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Najczęściej występujące bigramy (top 25):\n",
            "('dont', 'know'): 124\n",
            "('last', 'year'): 110\n",
            "('new', 'york'): 101\n",
            "('im', 'sure'): 87\n",
            "('gordon', 'banks'): 80\n",
            "('anonymous', 'ftp'): 80\n",
            "('banks', 'skepticism'): 73\n",
            "('skepticism', 'chastity'): 73\n",
            "('chastity', 'intellect'): 73\n",
            "('intellect', 'gebcadredslpittedu'): 72\n",
            "('gebcadredslpittedu', 'shameful'): 72\n",
            "('shameful', 'surrender'): 72\n",
            "('surrender', 'soon'): 72\n",
            "('dont', 'think'): 71\n",
            "('would', 'like'): 69\n",
            "('space', 'station'): 68\n",
            "('years', 'ago'): 59\n",
            "('image', 'processing'): 56\n",
            "('anyone', 'know'): 55\n",
            "('medical', 'newsletter'): 55\n",
            "('space', 'shuttle'): 54\n",
            "('hicnet', 'medical'): 54\n",
            "('newsletter', 'page'): 54\n",
            "('volume', 'number'): 54\n",
            "('number', 'april'): 54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 5 Analiza trigramów"
      ],
      "metadata": {
        "id": "0CFbfyIJ1Ltj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "# Wybieramy kilka kategorii do analizy\n",
        "categories = ['sci.med', 'sci.space', 'rec.sport.baseball', 'comp.graphics']\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "documents = newsgroups.data\n",
        "\n",
        "print(f\"Pobrano {len(documents)} dokumentów.\")\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
        "    return tokens\n",
        "\n",
        "processed_documents = [preprocess(doc) for doc in documents]\n",
        "\n",
        "# Generowanie trigramów z użycie ntlk\n",
        "trigramy = []\n",
        "for dokument in processed_documents:\n",
        "  ngrams_temp = ngrams(dokument, 3)\n",
        "  for ngram in ngrams_temp:\n",
        "    trigramy.append(ngram)\n",
        "\n",
        "licznik_triramow = Counter(trigramy)\n",
        "\n",
        "# Wyświetlanie n najczęściej występujących trigramów\n",
        "print(f\"Najczęściej występujące trigramy (top 25):\")\n",
        "for trigram, liczba_wystapien in licznik_triramow.most_common(25):\n",
        "    print(f\"{trigram}: {liczba_wystapien}\")"
      ],
      "metadata": {
        "id": "lV6Z_u_c1OX3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}